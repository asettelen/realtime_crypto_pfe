{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5810ab15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting httplib2==0.12.0\n",
      "  Downloading httplib2-0.12.0.tar.gz (218 kB)\n",
      "\u001b[K     |████████████████████████████████| 218 kB 8.5 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: httplib2\n",
      "  Building wheel for httplib2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for httplib2: filename=httplib2-0.12.0-py3-none-any.whl size=93465 sha256=fa73c0a659f4e63188a03a081a9a2246b0c732d8b55c22a78c1ed06076c0e6c4\n",
      "  Stored in directory: /root/.cache/pip/wheels/0d/e7/b6/0dd30343ceca921cfbd91f355041bd9c69e0f40b49f25b7b8a\n",
      "Successfully built httplib2\n",
      "\u001b[31mERROR: google-auth-httplib2 0.1.0 has requirement httplib2>=0.15.0, but you'll have httplib2 0.12.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-api-python-client 2.3.0 has requirement httplib2<1dev,>=0.15.0, but you'll have httplib2 0.12.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: httplib2\n",
      "  Attempting uninstall: httplib2\n",
      "    Found existing installation: httplib2 0.19.1\n",
      "    Uninstalling httplib2-0.19.1:\n",
      "      Successfully uninstalled httplib2-0.19.1\n",
      "Successfully installed httplib2-0.12.0\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/root/apache-beam-2.25.0/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install httplib2==0.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60b8b744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==0.13.0\n",
      "# Editable install with no version control (apache-beam==2.25.0)\n",
      "-e /root/apache-beam-2.25.0/packages/beam/sdks/python\n",
      "argon2-cffi==20.1.0\n",
      "astunparse==1.6.3\n",
      "async-generator==1.10\n",
      "attrs==21.2.0\n",
      "avro-python3==1.8.2\n",
      "backcall==0.2.0\n",
      "bleach==3.3.1\n",
      "cachetools==4.2.2\n",
      "certifi==2021.5.30\n",
      "cffi==1.14.6\n",
      "charset-normalizer==2.0.3\n",
      "crcmod==1.7\n",
      "cycler==0.10.0\n",
      "Cython==0.29.13\n",
      "debugpy==1.3.0\n",
      "decorator==5.0.9\n",
      "defusedxml==0.7.1\n",
      "dill==0.3.1.1\n",
      "docopt==0.6.2\n",
      "entrypoints==0.3\n",
      "facets-overview==1.0.0\n",
      "fastavro==1.0.0.post1\n",
      "fasteners==0.16.3\n",
      "future==0.18.2\n",
      "gast==0.3.3\n",
      "google-api-core==1.22.0\n",
      "google-apitools==0.5.31\n",
      "google-auth==1.33.1\n",
      "google-auth-oauthlib==0.4.4\n",
      "google-cloud-bigquery==1.26.1\n",
      "google-cloud-bigtable==1.0.0\n",
      "google-cloud-build==2.0.0\n",
      "google-cloud-core==1.4.1\n",
      "google-cloud-datastore==1.7.4\n",
      "google-cloud-dlp==1.0.0\n",
      "google-cloud-language==1.3.0\n",
      "google-cloud-pubsub==1.0.2\n",
      "google-cloud-spanner==1.19.1\n",
      "google-cloud-videointelligence==1.16.1\n",
      "google-cloud-vision==1.0.0\n",
      "google-crc32c==1.1.2\n",
      "google-pasta==0.2.0\n",
      "google-resumable-media==1.3.1\n",
      "googleapis-common-protos==1.53.0\n",
      "grpc-google-iam-v1==0.12.3\n",
      "grpcio==1.38.1\n",
      "grpcio-gcp==0.2.2\n",
      "guppy3==3.0.9\n",
      "h5py==2.10.0\n",
      "hdfs==2.5.8\n",
      "httplib2==0.12.0\n",
      "idna==3.2\n",
      "importlib-metadata==4.6.1\n",
      "ipykernel==5.5.5\n",
      "ipython==7.25.0\n",
      "ipython-genutils==0.2.0\n",
      "ipywidgets==7.6.3\n",
      "jedi==0.18.0\n",
      "Jinja2==3.0.1\n",
      "jsonschema==3.2.0\n",
      "jupyter-client==6.1.12\n",
      "jupyter-core==4.7.1\n",
      "jupyterlab-pygments==0.1.2\n",
      "jupyterlab-widgets==1.0.0\n",
      "Keras-Preprocessing==1.1.2\n",
      "kiwisolver==1.3.1\n",
      "libcst==0.3.19\n",
      "Markdown==3.3.4\n",
      "MarkupSafe==2.0.1\n",
      "matplotlib==3.4.2\n",
      "matplotlib-inline==0.1.2\n",
      "mistune==0.8.4\n",
      "mmh3==2.5.1\n",
      "mock==2.0.0\n",
      "mypy-extensions==0.4.3\n",
      "nbclient==0.5.3\n",
      "nbconvert==6.1.0\n",
      "nbformat==5.1.3\n",
      "nest-asyncio==1.5.1\n",
      "nose==1.3.7\n",
      "notebook==6.4.0\n",
      "numpy==1.17.3\n",
      "oauth2client==4.1.3\n",
      "oauthlib==3.1.1\n",
      "opt-einsum==3.3.0\n",
      "packaging==21.0\n",
      "pandas==0.25.2\n",
      "pandocfilters==1.4.3\n",
      "parso==0.8.2\n",
      "pbr==5.6.0\n",
      "pexpect==4.8.0\n",
      "pickleshare==0.7.5\n",
      "Pillow==8.3.1\n",
      "prometheus-client==0.11.0\n",
      "prompt-toolkit==3.0.19\n",
      "proto-plus==1.19.0\n",
      "protobuf==3.12.2\n",
      "protorpc==0.12.0\n",
      "ptyprocess==0.7.0\n",
      "pyarrow==0.16.0\n",
      "pyasn1==0.4.8\n",
      "pyasn1-modules==0.2.8\n",
      "pycparser==2.20\n",
      "pydot==1.4.1\n",
      "Pygments==2.9.0\n",
      "PyHamcrest==1.10.1\n",
      "pymongo==3.9.0\n",
      "pyparsing==2.4.7\n",
      "pyrsistent==0.18.0\n",
      "python-dateutil==2.8.2\n",
      "python-gflags==3.0.6\n",
      "pytz==2019.3\n",
      "PyYAML==5.4.1\n",
      "pyzmq==22.1.0\n",
      "requests==2.26.0\n",
      "requests-oauthlib==1.3.0\n",
      "rsa==4.7.2\n",
      "scipy==1.4.1\n",
      "Send2Trash==1.7.1\n",
      "six==1.16.0\n",
      "sql==0.4.0\n",
      "tenacity==8.0.1\n",
      "tensorboard==2.5.0\n",
      "tensorboard-data-server==0.6.1\n",
      "tensorboard-plugin-wit==1.8.0\n",
      "tensorflow==2.3.0\n",
      "tensorflow-estimator==2.3.0\n",
      "termcolor==1.1.0\n",
      "terminado==0.10.1\n",
      "testpath==0.5.0\n",
      "timeloop==1.0.2\n",
      "tornado==6.1\n",
      "traitlets==5.0.5\n",
      "typing-extensions==3.7.4.3\n",
      "typing-inspect==0.7.1\n",
      "urllib3==1.26.6\n",
      "wcwidth==0.2.5\n",
      "webencodings==0.5.1\n",
      "Werkzeug==2.0.1\n",
      "widgetsnbextension==3.5.1\n",
      "wrapt==1.12.1\n",
      "zipp==3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29fb4f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import traceback\n",
    "import google.auth\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam import WithKeys, GroupByKey\n",
    "from apache_beam import FlatMap, Map, ParDo, Flatten, Filter, GroupBy\n",
    "from apache_beam import Values, CombineGlobally, CombinePerKey\n",
    "from apache_beam import pvalue, window, WindowInto\n",
    "from apache_beam.options import pipeline_options\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.transforms.combiners import Top, Mean, Count\n",
    "from apache_beam.io.textio import ReadFromText, WriteToText\n",
    "from apache_beam.io.gcp.pubsub import ReadFromPubSub\n",
    "from apache_beam.io.gcp.bigquery import BigQueryDisposition, WriteToBigQuery\n",
    "\n",
    "from apache_beam.runners import DataflowRunner\n",
    "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
    "import apache_beam.runners.interactive.interactive_beam as ib\n",
    "\n",
    "from apache_beam.testing.util import assert_that, is_empty, equal_to\n",
    "\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "from apache_beam.options.pipeline_options import StandardOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6412be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://temp_crypto_batch_verdant_cargo_321713'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = google.auth.default()[1]\n",
    "bucket = 'gs://' + 'temp_crypto_batch_' + google.auth.default()[1].replace('-','_')\n",
    "bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71672cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bbb1624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setuptools import setup, find_packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0b70c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile setup.py\n",
    "\n",
    "from setuptools import setup, find_packages\n",
    "\n",
    "setup(\n",
    "    name=\"dataflow_pipeline_dependencies\",\n",
    "    version=\"0.1\",\n",
    "    install_requires=[\n",
    "   'numpy==1.17.3', 'pandas==1.2.4', 'pystan==2.19.1.1', 'prophet==1.0.1', 'pytz==2019.3'\n",
    "    ],\n",
    "    packages = find_packages()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "133df99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  `temp_crypto_batch.batch_hour_interpo`\n",
    "WHERE \n",
    "  tumble <= (SELECT TIMESTAMP_SUB(max(tumble), INTERVAL 5 DAY) FROM `temp_crypto_batch.batch_hour_interpo`)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ba3828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_parameters = [\n",
    "    '--project', project,\n",
    "    '--staging_location', \"%s/staging_bq_notebook\" % bucket,\n",
    "    '--temp_location', \"%s/temp_bq_notebook\" % bucket,\n",
    "    \"--setup_file\", './setup.py', \n",
    "    \"--region\", \"us-central1\",\n",
    "    \"--template_location\", bucket + '/prediction/templates/CUSTOM_TEMPLATE_DATAFLOW'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6670fcfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        var import_html = () => {\n",
       "          ['https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html'].forEach(href => {\n",
       "            var link = document.createElement('link');\n",
       "            link.rel = 'import'\n",
       "            link.href = href;\n",
       "            document.head.appendChild(link);\n",
       "          });\n",
       "        }\n",
       "        if ('import' in document.createElement('link')) {\n",
       "          import_html();\n",
       "        } else {\n",
       "          var webcomponentScript = document.createElement('script');\n",
       "          webcomponentScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js';\n",
       "          webcomponentScript.type = 'text/javascript';\n",
       "          webcomponentScript.onload = function(){\n",
       "            import_html();\n",
       "          };\n",
       "          document.head.appendChild(webcomponentScript);\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/apache-beam-2.25.0/packages/beam/sdks/python/apache_beam/io/gcp/bigquery.py:1971: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  temp_location = pcoll.pipeline.options.view_as(\n",
      "/root/apache-beam-2.25.0/packages/beam/sdks/python/apache_beam/io/gcp/bigquery_file_loads.py:900: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  temp_location = p.options.view_as(GoogleCloudOptions).temp_location\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/root/apache-beam-2.25.0/bin/python', 'setup.py', 'sdist', '--dist-dir', '/tmp/tmp5xz3nbyi']\n",
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/root/apache-beam-2.25.0/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmp5xz3nbyi', 'apache-beam==2.25.0', '--no-deps', '--no-binary', ':all:']\n",
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI: dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/root/apache-beam-2.25.0/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmp5xz3nbyi', 'apache-beam==2.25.0', '--no-deps', '--only-binary', ':all:', '--python-version', '37', '--implementation', 'cp', '--abi', 'cp37m', '--platform', 'manylinux1_x86_64']\n",
      "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI: apache_beam-2.25.0-cp37-cp37m-manylinux1_x86_64.whl\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "INFO:root:Using Python SDK docker image: apache/beam_python3.7_sdk:2.25.0. If the image is not available at local, we will try to pull from hub.docker.com\n",
      "INFO:apache_beam.internal.gcp.auth:Setting socket default timeout to 60 seconds.\n",
      "INFO:apache_beam.internal.gcp.auth:socket default timeout is 60.0 seconds.\n",
      "INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://temp_crypto_batch_verdant_cargo_321713/staging_bq_notebook/beamapp-root-0804104200-027177.1628073720.027349/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://temp_crypto_batch_verdant_cargo_321713/staging_bq_notebook/beamapp-root-0804104200-027177.1628073720.027349/pipeline.pb in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://temp_crypto_batch_verdant_cargo_321713/staging_bq_notebook/beamapp-root-0804104200-027177.1628073720.027349/workflow.tar.gz...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://temp_crypto_batch_verdant_cargo_321713/staging_bq_notebook/beamapp-root-0804104200-027177.1628073720.027349/workflow.tar.gz in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://temp_crypto_batch_verdant_cargo_321713/staging_bq_notebook/beamapp-root-0804104200-027177.1628073720.027349/dataflow_python_sdk.tar...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://temp_crypto_batch_verdant_cargo_321713/staging_bq_notebook/beamapp-root-0804104200-027177.1628073720.027349/dataflow_python_sdk.tar in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://temp_crypto_batch_verdant_cargo_321713/staging_bq_notebook/beamapp-root-0804104200-027177.1628073720.027349/apache_beam-2.25.0-cp37-cp37m-manylinux1_x86_64.whl...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://temp_crypto_batch_verdant_cargo_321713/staging_bq_notebook/beamapp-root-0804104200-027177.1628073720.027349/apache_beam-2.25.0-cp37-cp37m-manylinux1_x86_64.whl in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://temp_crypto_batch_verdant_cargo_321713/prediction/templates/CUSTOM_TEMPLATE_DATAFLOW...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://temp_crypto_batch_verdant_cargo_321713/prediction/templates/CUSTOM_TEMPLATE_DATAFLOW in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:A template was just created at location gs://temp_crypto_batch_verdant_cargo_321713/prediction/templates/CUSTOM_TEMPLATE_DATAFLOW\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<DataflowPipelineResult None at 0x7f670987f710>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class KeepDoFn(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        return [{\n",
    "            'symbol': element['symbol'],\n",
    "            'ds': element['tumble'],\n",
    "            'y' : element['intrp']\n",
    "    }]\n",
    "\n",
    "###########################\n",
    "    \n",
    "class KeepDoFnAfterGrouping(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        return [{\n",
    "            'symbol': element[0],\n",
    "            'data': element[1]\n",
    "    }]\n",
    "    \n",
    "###########################\n",
    "\n",
    "class Prophet_M(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        import pandas\n",
    "        #from datetime import datetime\n",
    "        import numpy as np\n",
    "        from prophet import Prophet\n",
    "        \n",
    "        symbol = element['symbol']\n",
    "        data = element['data']\n",
    "        \n",
    "        data = pandas.DataFrame(data)\n",
    "        data['ds'] = data['ds'].dt.tz_localize(None)\n",
    "        \n",
    "        ##############\n",
    "        \n",
    "        max_datetime = max(data['ds'])\n",
    "        # Extract training data\n",
    "        train_data = data[data['ds'] <= max_datetime]\n",
    "        \n",
    "        #############\n",
    "        \n",
    "        m = Prophet(seasonality_mode=\"multiplicative\") \n",
    "            \n",
    "        #############\n",
    "        \n",
    "        \"\"\"Fit the model using the training data.\"\"\"\n",
    "        m.fit(train_data)\n",
    "        \n",
    "        ############\n",
    "        \n",
    "        future = m.make_future_dataframe(periods=120, freq='H')      \n",
    "        forecast = m.predict(future)\n",
    "        \n",
    "        ###########\n",
    "        \n",
    "        forecast=pandas.merge(forecast, train_data, on='ds', how='left')\n",
    "        \n",
    "        x = np.array(symbol)\n",
    "        forecast['symbol']=np.repeat(x, len(forecast), axis=0)\n",
    "        \n",
    "        ############\n",
    "        \n",
    "        forecast_dict = forecast.to_dict('records')\n",
    "        \n",
    "        return [{\n",
    "        'symbol' : symbol,\n",
    "        'forecast': forecast_dict   \n",
    "        }]\n",
    "    \n",
    "########################\n",
    "\n",
    "class Expand_Predictions_M(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        import pandas, pytz, math\n",
    "        \n",
    "        return [{\n",
    "        'symbol' : element['symbol'],\n",
    "        'ds': pytz.utc.localize(element['ds']).to_pydatetime(),\n",
    "        'y': element['y'] if not(math.isnan(element['y'])) else None,\n",
    "        'yhat': element['yhat'],\n",
    "        'yhat_lower': element['yhat_lower'], \n",
    "        'yhat_upper': element['yhat_upper']\n",
    "        }]\n",
    "    \n",
    "#######################\n",
    "\n",
    "table = \"{}:temp_crypto_batch.test_prophet_global\".format(project)\n",
    "schema = \"symbol:STRING, ds:TIMESTAMP, y:FLOAT, yhat:FLOAT, yhat_lower:FLOAT, yhat_upper:FLOAT\"\n",
    "\n",
    "p = beam.Pipeline(\"DataFlowRunner\", argv=pipeline_parameters)\n",
    "create = (p | \"Create\" >>  beam.io.ReadFromBigQuery(query=query,use_standard_sql=True)\n",
    "            | \"Map Keys\" >> beam.ParDo(KeepDoFn())\n",
    "            | \"Add Keys\" >> WithKeys(lambda x: x[\"symbol\"])\n",
    "            | GroupByKey()\n",
    "            | \"Rename After Grouping\" >> beam.ParDo(KeepDoFnAfterGrouping())\n",
    "            | \"Prophet Data\" >> beam.ParDo(Prophet_M())\n",
    "            | \"Flat Map Data\" >> FlatMap(lambda x: x[\"forecast\"])\n",
    "            | \"Expand Predictions\" >> beam.ParDo(Expand_Predictions_M())\n",
    "            | \"Write To BigQuery\" >> WriteToBigQuery(table=table, schema=schema,\n",
    "                                  create_disposition=BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "                                  write_disposition=BigQueryDisposition.WRITE_TRUNCATE))\n",
    "\n",
    "p.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "7. Apache Beam 2.25.0 for Python 3",
   "language": "python",
   "name": "7-apache-beam-2.25.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
