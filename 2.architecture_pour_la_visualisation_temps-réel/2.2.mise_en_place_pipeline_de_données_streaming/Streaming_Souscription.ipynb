{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "283d2d35",
   "metadata": {},
   "source": [
    "Ici se trouve le code correspondant à la partie streaming du schéma d'architecture du cas d'usage d'implémentation de d'analyse data de cryptomonnaie. \n",
    "\n",
    "Il s'agit de récupérer des données publiées en temps réel dans un topic pub/sub grâce à une subscription préalablement créée.\n",
    "\n",
    "Chaque message reçu par la subscription sera alors préparé par une pipeline de données (le même traitement que dans le cadre des données récupérées en batch car provenant de la même source de données). \n",
    "\n",
    "De la même manière que pour le batch, nous lancerons et exécuterons d'abord la pipeline de données en local sur la VM (grâce à interactiverunner()) puis exécuterons la pipeline sur le cloud grâce à dataflowrunner()).  \n",
    "\n",
    "Le dataflow exporte toutes les données traitées dans une table de BigQuery. \n",
    "Ces données sont prêtes à être visualisé depuis un outil de visualisation de données au choix (DataStudio, Grafana, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e649c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options import pipeline_options\n",
    "from apache_beam.io.gcp.pubsub import ReadFromPubSub\n",
    "from apache_beam.io.gcp.bigquery import BigQueryDisposition, WriteToBigQuery\n",
    "from apache_beam.io import WriteToText\n",
    "\n",
    "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
    "import apache_beam.runners.interactive.interactive_beam as ib\n",
    "from apache_beam.runners import DataflowRunner\n",
    "\n",
    "import google.auth\n",
    "\n",
    "#from utils.utils import publish_to_topic\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e32e2853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'verdant-cargo-321713'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The project will be used for creating a subscription to the Pub/Sub topic and for the Dataflow pipeline\n",
    "project = google.auth.default()[1]\n",
    "project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "264b1569",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = pipeline_options.PipelineOptions(\n",
    "    streaming=True,\n",
    "    project=project\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b850d1d",
   "metadata": {},
   "source": [
    "Si le batch a déjà été exécuté, pas besoin d'exécuter la ligne suivante car la dataset aura déjà été créée "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26b80b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!bq mk --location US --dataset temp_crypto_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759cc350",
   "metadata": {},
   "source": [
    "2. Interactive Runner "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa08a86b",
   "metadata": {},
   "source": [
    "Lançons et exécutons la pipeline de données localement depuis la VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9938aee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c783c1db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        var import_html = () => {\n",
       "          ['https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html'].forEach(href => {\n",
       "            var link = document.createElement('link');\n",
       "            link.rel = 'import'\n",
       "            link.href = href;\n",
       "            document.head.appendChild(link);\n",
       "          });\n",
       "        }\n",
       "        if ('import' in document.createElement('link')) {\n",
       "          import_html();\n",
       "        } else {\n",
       "          var webcomponentScript = document.createElement('script');\n",
       "          webcomponentScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js';\n",
       "          webcomponentScript.type = 'text/javascript';\n",
       "          webcomponentScript.onload = function(){\n",
       "            import_html();\n",
       "          };\n",
       "          document.head.appendChild(webcomponentScript);\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n",
       "            <div class=\"alert alert-info\">You have changed recording duration from 60.0 seconds to 600.0 seconds. To allow new data to be recorded for the updated duration the next time a PCollection is evaluated or the pipeline is executed, please invoke ib.recordings.stop, ib.recordings.clear and ib.recordings.record.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Temps durant lequel la souscription sera active localement et alimentera le dataflow\n",
    "ib.options.recording_duration = '10m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee8b115c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class KeepDoFn(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        return [{\n",
    "            'timestamp': element['block_timestamp_truncated'],\n",
    "            'request': str(element['oracle_request_id']),\n",
    "            'symbol' : element['symbol'],\n",
    "            'rate' : element['rate'],  \n",
    "            'date_str' : pd.to_datetime(element['block_timestamp'], format='%Y-%m-%dT%H:%M:%S.%fZ').strftime(\"%Y%m%d%H%M%S\")\n",
    "    }]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d370878",
   "metadata": {},
   "source": [
    "Commande suivante à n'exécuter que lors de la première insertion en streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d2e7f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table verdant-cargo-321713.temp_crypto_batch.streaming_timestamp\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Construct a BigQuery client object.\n",
    "client = bigquery.Client()\n",
    "\n",
    "# TODO(developer): Set table_id to the ID of the table to create.\n",
    "table_id = \"{}.temp_crypto_batch.streaming_timestamp\".format(project)\n",
    "\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"timestamp\", \"TIMESTAMP\"),\n",
    "    bigquery.SchemaField(\"request\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"symbol\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"rate\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"date_str\", \"STRING\"),\n",
    "]\n",
    "\n",
    "table = bigquery.Table(table_id, schema=schema)\n",
    "table = client.create_table(table)  # Make an API request.\n",
    "print(\n",
    "    \"Created table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c595402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://verdant_cargo_321713_bigquery_to_pubsub_temp'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket = \"gs://{project}_bigquery_to_pubsub_temp\".format(project=project.replace(\"-\", \"_\"))\n",
    "bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2753b7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def streaming_pipeline(project, region=\"us-central1\"):\n",
    "    \n",
    "    table = \"{}:temp_crypto_batch.streaming_timestamp\".format(project)\n",
    "    schema = \"timestamp:TIMESTAMP, request:STRING, symbol:STRING, rate:FLOAT, date_str:STRING\"\n",
    "    \n",
    "    topic = \"projects/{}/topics/bigquery-to-pubsub-test0\".format(project)\n",
    "    bucket = \"gs://{project}_bigquery_to_pubsub_temp\".format(project=project.replace(\"-\", \"_\"))\n",
    "    \n",
    "    options = PipelineOptions(\n",
    "        streaming=True,\n",
    "        project=project,\n",
    "        region=region,\n",
    "        staging_location=\"%s/staging\" % bucket,\n",
    "        temp_location=\"%s/temp\" % bucket\n",
    "    )\n",
    "\n",
    "    p = beam.Pipeline(InteractiveRunner(), options=options)\n",
    "\n",
    "    pubsub = (p | \"Read Topic\" >> ReadFromPubSub(topic=topic)\n",
    "                | \"To Dict\" >> beam.Map(json.loads)\n",
    "                | \"Format\" >> beam.ParDo(KeepDoFn())\n",
    "                | \"Write To BigQuery\" >> WriteToBigQuery(table=table, schema=schema,\n",
    "                                  create_disposition=BigQueryDisposition.CREATE_NEVER,\n",
    "                                  write_disposition=BigQueryDisposition.WRITE_APPEND))\n",
    "    \n",
    "    return ib.show(pubsub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33571aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n",
       "            <div id=\"progress_indicator_dc3eed33da1fda01f2b567a0b311399e\" class=\"spinner-border text-info\" role=\"status\">\n",
       "            </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n",
       "            <div class=\"alert alert-info\">Interactive Beam has detected unbounded sources in your pipeline. In order to have a deterministic replay, a segment of data will be recorded from all sources for 600.0 seconds or until a total of 1.0GB have been written to disk.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "            .p-Widget.jp-OutputPrompt.jp-OutputArea-prompt:empty {\n",
       "              padding: 0;\n",
       "              border: 0;\n",
       "            }\n",
       "            .p-Widget.jp-RenderedJavaScript.jp-mod-trusted.jp-OutputArea-output:empty {\n",
       "              padding: 0;\n",
       "              border: 0;\n",
       "            }\n",
       "            </style>\n",
       "            <div id=\"no_data_table_df_5dbb6730a58bf5a09099eebea5523556\">No data to display.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.runners.direct.test_stream_impl:TestStream timed out waiting for new events from service. Stopping pipeline.\n",
      "ERROR:apache_beam.runners.direct.executor:Exception at bundle <apache_beam.runners.direct.bundle_factory._Bundle object at 0x7fc4654838c0>, due to an exception.\n",
      " Traceback (most recent call last):\n",
      "  File \"/root/apache-beam-2.25.0/packages/beam/sdks/python/apache_beam/runners/direct/executor.py\", line 382, in call\n",
      "    finish_state)\n",
      "  File \"/root/apache-beam-2.25.0/packages/beam/sdks/python/apache_beam/runners/direct/executor.py\", line 420, in attempt_call\n",
      "    evaluator.process_element(value)\n",
      "  File \"/root/apache-beam-2.25.0/packages/beam/sdks/python/apache_beam/runners/direct/transform_evaluator.py\", line 545, in process_element\n",
      "    events.append(next(self.event_stream))\n",
      "  File \"/root/apache-beam-2.25.0/packages/beam/sdks/python/apache_beam/runners/direct/test_stream_impl.py\", line 339, in events_from_rpc\n",
      "    raise e\n",
      "  File \"/root/apache-beam-2.25.0/packages/beam/sdks/python/apache_beam/runners/direct/test_stream_impl.py\", line 330, in events_from_rpc\n",
      "    event = channel.get(timeout=30)\n",
      "  File \"/opt/conda/lib/python3.7/queue.py\", line 178, in get\n",
      "    raise Empty\n",
      "_queue.Empty\n",
      "\n",
      "ERROR:apache_beam.runners.direct.executor:Exception at bundle <apache_beam.runners.direct.bundle_factory._Bundle object at 0x7fc4654838c0>, due to an exception.\n",
      " Traceback (most recent call last):\n",
      "  File \"/root/apache-beam-2.25.0/packages/beam/sdks/python/apache_beam/runners/direct/executor.py\", line 382, in call\n",
      "    finish_state)\n",
      "  File \"/root/apache-beam-2.25.0/packages/beam/sdks/python/apache_beam/runners/direct/executor.py\", line 420, in attempt_call\n",
      "    evaluator.process_element(value)\n",
      "  File \"/root/apache-beam-2.25.0/packages/beam/sdks/python/apache_beam/runners/direct/transform_evaluator.py\", line 545, in process_element\n",
      "    events.append(next(self.event_stream))\n",
      "ValueError: generator already executing\n",
      "\n",
      "ERROR:apache_beam.runners.direct.executor:Exception at bundle <apache_beam.runners.direct.bundle_factory._Bundle object at 0x7fc4654838c0>, due to an exception.\n",
      " Traceback (most recent call last):\n",
      "  File \"/root/apache-beam-2.25.0/packages/beam/sdks/python/apache_beam/runners/direct/executor.py\", line 382, in call\n",
      "    finish_state)\n",
      "  File \"/root/apache-beam-2.25.0/packages/beam/sdks/python/apache_beam/runners/direct/executor.py\", line 420, in attempt_call\n",
      "    evaluator.process_element(value)\n",
      "  File \"/root/apache-beam-2.25.0/packages/beam/sdks/python/apache_beam/runners/direct/transform_evaluator.py\", line 545, in process_element\n",
      "    events.append(next(self.event_stream))\n",
      "ValueError: generator already executing\n",
      "\n",
      "ERROR:apache_beam.runners.direct.executor:Exception at bundle <apache_beam.runners.direct.bundle_factory._Bundle object at 0x7fc4654838c0>, due to an exception.\n",
      " Traceback (most recent call last):\n",
      "  File \"/root/apache-beam-2.25.0/packages/beam/sdks/python/apache_beam/runners/direct/executor.py\", line 382, in call\n",
      "    finish_state)\n",
      "  File \"/root/apache-beam-2.25.0/packages/beam/sdks/python/apache_beam/runners/direct/executor.py\", line 420, in attempt_call\n",
      "    evaluator.process_element(value)\n",
      "  File \"/root/apache-beam-2.25.0/packages/beam/sdks/python/apache_beam/runners/direct/transform_evaluator.py\", line 545, in process_element\n",
      "    events.append(next(self.event_stream))\n",
      "ValueError: generator already executing\n",
      "\n",
      "ERROR:apache_beam.runners.direct.executor:Giving up after 4 attempts.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n",
       "            <div class=\"alert alert-danger\">Timed out waiting for cache file for PCollection `6d8752b145` to be available with path /tmp/it-_kyush1q140481507470864/full/d9c86db848-140481498871312-140481498668624-140481507470864.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "            $(\"#progress_indicator_dc3eed33da1fda01f2b567a0b311399e\").remove();\n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "            $(\"#progress_indicator_dc3eed33da1fda01f2b567a0b311399e\").remove();\n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "streaming_pipeline(project)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4735fe",
   "metadata": {},
   "source": [
    "IMPORTANT : Arrêtez-vous ici et reprenez la suite des instructions du Mardown README.md <br>\n",
    "La suite sera faite après."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb716eda",
   "metadata": {},
   "source": [
    "3. DataflowRunner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2898e8",
   "metadata": {},
   "source": [
    "Cette fois-ci, testons le dataflow directement sur le cloud avec dataflowrunner(). Le code va donc se lancer depuis la VM et s'exécuter sur Dataflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3691f361",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07c0257a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setuptools import setup, find_packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "614c9f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile setup.py\n",
    "\n",
    "from setuptools import setup, find_packages\n",
    "\n",
    "setup(\n",
    "    name=\"dataflow_pipeline_dependencies\",\n",
    "    version=\"0.1\",\n",
    "    install_requires=[\n",
    "   'pandas==0.25.2',\n",
    "    ],\n",
    "    packages = find_packages()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0469122e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"gs://{project}_bigquery_to_pubsub_temp\".format(project=project.replace(\"-\", \"_\"))\n",
    "pipeline_parameters = [\n",
    "    '--streaming', True,\n",
    "    '--project', project,\n",
    "    '--staging_location', \"%s/staging\" % bucket,\n",
    "    '--temp_location', \"%s/temp\" % bucket,\n",
    "    \"--setup_file\", './setup.py', \n",
    "    \"--region\", \"us-central1\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3734edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/root/apache-beam-2.25.0/bin/python', 'setup.py', 'sdist', '--dist-dir', '/tmp/tmp5y89uotj']\n",
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/root/apache-beam-2.25.0/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmp5y89uotj', 'apache-beam==2.25.0', '--no-deps', '--no-binary', ':all:']\n",
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI: dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/root/apache-beam-2.25.0/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmp5y89uotj', 'apache-beam==2.25.0', '--no-deps', '--only-binary', ':all:', '--python-version', '37', '--implementation', 'cp', '--abi', 'cp37m', '--platform', 'manylinux1_x86_64']\n",
      "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI: apache_beam-2.25.0-cp37-cp37m-manylinux1_x86_64.whl\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "INFO:root:Using Python SDK docker image: apache/beam_python3.7_sdk:2.25.0. If the image is not available at local, we will try to pull from hub.docker.com\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://verdant_cargo_321713_bigquery_to_pubsub_temp/staging/beamapp-root-0803093639-841908.1627983399.842019/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://verdant_cargo_321713_bigquery_to_pubsub_temp/staging/beamapp-root-0803093639-841908.1627983399.842019/pipeline.pb in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://verdant_cargo_321713_bigquery_to_pubsub_temp/staging/beamapp-root-0803093639-841908.1627983399.842019/workflow.tar.gz...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://verdant_cargo_321713_bigquery_to_pubsub_temp/staging/beamapp-root-0803093639-841908.1627983399.842019/workflow.tar.gz in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://verdant_cargo_321713_bigquery_to_pubsub_temp/staging/beamapp-root-0803093639-841908.1627983399.842019/dataflow_python_sdk.tar...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://verdant_cargo_321713_bigquery_to_pubsub_temp/staging/beamapp-root-0803093639-841908.1627983399.842019/dataflow_python_sdk.tar in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://verdant_cargo_321713_bigquery_to_pubsub_temp/staging/beamapp-root-0803093639-841908.1627983399.842019/apache_beam-2.25.0-cp37-cp37m-manylinux1_x86_64.whl...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://verdant_cargo_321713_bigquery_to_pubsub_temp/staging/beamapp-root-0803093639-841908.1627983399.842019/apache_beam-2.25.0-cp37-cp37m-manylinux1_x86_64.whl in 2 seconds.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-f68f5338-cb52-4a70-afa2-84c4ef63982a.json']\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-f68f5338-cb52-4a70-afa2-84c4ef63982a.json']\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " createTime: '2021-08-03T09:36:43.995136Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2021-08-03_02_36_42-9512594698141357029'\n",
      " location: 'us-central1'\n",
      " name: 'beamapp-root-0803093639-841908'\n",
      " projectId: 'verdant-cargo-321713'\n",
      " stageStates: []\n",
      " startTime: '2021-08-03T09:36:43.995136Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_STREAMING, 2)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2021-08-03_02_36_42-9512594698141357029]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Submitted job: 2021-08-03_02_36_42-9512594698141357029\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobs/us-central1/2021-08-03_02_36_42-9512594698141357029?project=verdant-cargo-321713\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<DataflowPipelineResult <Job\n",
       " createTime: '2021-08-03T09:36:43.995136Z'\n",
       " currentStateTime: '1970-01-01T00:00:00Z'\n",
       " id: '2021-08-03_02_36_42-9512594698141357029'\n",
       " location: 'us-central1'\n",
       " name: 'beamapp-root-0803093639-841908'\n",
       " projectId: 'verdant-cargo-321713'\n",
       " stageStates: []\n",
       " startTime: '2021-08-03T09:36:43.995136Z'\n",
       " steps: []\n",
       " tempFiles: []\n",
       " type: TypeValueValuesEnum(JOB_TYPE_STREAMING, 2)> at 0x7fc464925690>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class KeepDoFn(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        import pandas as pd\n",
    "\n",
    "        return [{\n",
    "            'timestamp': element['block_timestamp_truncated'],\n",
    "            'request': str(element['oracle_request_id']),\n",
    "            'symbol' : element['symbol'],\n",
    "            'rate' : element['rate'],  \n",
    "            'date_str' : pd.to_datetime(element['block_timestamp'], format='%Y-%m-%dT%H:%M:%S.%fZ').strftime(\"%Y%m%d%H%M%S\")\n",
    "    }]\n",
    "\n",
    "project = google.auth.default()[1]\n",
    "table = \"{}:temp_crypto_batch.streaming_timestamp\".format(project)\n",
    "schema = \"timestamp:TIMESTAMP, request:STRING, symbol:STRING, rate:FLOAT, date_str:STRING\"\n",
    "topic = \"projects/{}/topics/bigquery-to-pubsub-test0\".format(project)\n",
    "bucket = \"gs://{project}_bigquery_to_pubsub_temp\".format(project=project.replace(\"-\", \"_\"))\n",
    "streaming = True\n",
    "\n",
    "options = PipelineOptions(\n",
    "        streaming=True,\n",
    "        project=project,\n",
    "        region=\"us-central1\",\n",
    "        staging_location=\"%s/staging\" % bucket,\n",
    "        temp_location=\"%s/temp\" % bucket, \n",
    "        setup_file=\"./setup.py\" \n",
    ")\n",
    "\n",
    "#p = beam.Pipeline(\"DataFlowRunner\", argv=pipeline_parameters)\n",
    "p = beam.Pipeline(DataflowRunner(), options=options)\n",
    "\n",
    "pubsub = (p | \"Read Topic\" >> ReadFromPubSub(topic=topic)\n",
    "                | \"To Dict\" >> beam.Map(json.loads)\n",
    "                | \"Format\" >> beam.ParDo(KeepDoFn())\n",
    "                | \"Write To BigQuery\" >> WriteToBigQuery(table=table, schema=schema,\n",
    "                                  create_disposition=BigQueryDisposition.CREATE_NEVER,\n",
    "                                  write_disposition=BigQueryDisposition.WRITE_APPEND))\n",
    "    \n",
    "p.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82aad3f9",
   "metadata": {},
   "source": [
    "Penser à stopper la pipeline pour réduire les coûts lorsque le streaming se termine (depuis la console, sur le job, choisir STOP puis DRAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af49deab",
   "metadata": {},
   "source": [
    "Revenir au markdown pour la suite des commandes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf33931",
   "metadata": {},
   "source": [
    "4. Automatisation avec Cloud Functions : Création du template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da9074b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options import pipeline_options\n",
    "from apache_beam.io.gcp.pubsub import ReadFromPubSub\n",
    "from apache_beam.io.gcp.bigquery import BigQueryDisposition, WriteToBigQuery\n",
    "from apache_beam.io import WriteToText\n",
    "\n",
    "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
    "import apache_beam.runners.interactive.interactive_beam as ib\n",
    "from apache_beam.runners import DataflowRunner\n",
    "\n",
    "import google.auth\n",
    "\n",
    "#from utils.utils import publish_to_topic\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96bd2834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'verdant-cargo-321713'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The project will be used for creating a subscription to the Pub/Sub topic and for the Dataflow pipeline\n",
    "project = google.auth.default()[1]\n",
    "project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "486faf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = pipeline_options.PipelineOptions(\n",
    "    streaming=True,\n",
    "    project=project\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32120cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db6c7048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setuptools import setup, find_packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dcd4f9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile setup.py\n",
    "\n",
    "from setuptools import setup, find_packages\n",
    "\n",
    "setup(\n",
    "    name=\"dataflow_pipeline_dependencies\",\n",
    "    version=\"0.1\",\n",
    "    install_requires=[\n",
    "   'pandas==0.25.2',\n",
    "    ],\n",
    "    packages = find_packages()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8bd06ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'verdant-cargo-321713'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd99eed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"gs://{project}_bigquery_to_pubsub_temp\".format(project=project.replace(\"-\", \"_\"))\n",
    "pipeline_parameters = [\n",
    "    '--streaming', True,\n",
    "    '--project', project,\n",
    "    '--staging_location', \"%s/staging\" % bucket,\n",
    "    '--temp_location', \"%s/temp\" % bucket,\n",
    "    \"--setup_file\", './setup.py', \n",
    "    \"--region\", \"us-central1\",\n",
    "    \"--template_location\", \"gs://{project}_bigquery_to_pubsub_temp/templates/CUSTOM_TEMPLATE_DATAFLOW_STREAMING\".format(project=project.replace(\"-\", \"_\"))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f5489a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/root/apache-beam-2.25.0/bin/python', 'setup.py', 'sdist', '--dist-dir', '/tmp/tmpx3_9d6lb']\n",
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/root/apache-beam-2.25.0/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpx3_9d6lb', 'apache-beam==2.25.0', '--no-deps', '--no-binary', ':all:']\n",
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI: dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/root/apache-beam-2.25.0/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpx3_9d6lb', 'apache-beam==2.25.0', '--no-deps', '--only-binary', ':all:', '--python-version', '37', '--implementation', 'cp', '--abi', 'cp37m', '--platform', 'manylinux1_x86_64']\n",
      "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI: apache_beam-2.25.0-cp37-cp37m-manylinux1_x86_64.whl\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "INFO:root:Using Python SDK docker image: apache/beam_python3.7_sdk:2.25.0. If the image is not available at local, we will try to pull from hub.docker.com\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://verdant_cargo_321713_bigquery_to_pubsub_temp/staging/beamapp-root-0803114157-486015.1627990917.486127/pipeline.pb...\n",
      "INFO:oauth2client.transport:Refreshing due to a 401 (attempt 1/2)\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://verdant_cargo_321713_bigquery_to_pubsub_temp/staging/beamapp-root-0803114157-486015.1627990917.486127/pipeline.pb in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://verdant_cargo_321713_bigquery_to_pubsub_temp/staging/beamapp-root-0803114157-486015.1627990917.486127/workflow.tar.gz...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://verdant_cargo_321713_bigquery_to_pubsub_temp/staging/beamapp-root-0803114157-486015.1627990917.486127/workflow.tar.gz in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://verdant_cargo_321713_bigquery_to_pubsub_temp/staging/beamapp-root-0803114157-486015.1627990917.486127/dataflow_python_sdk.tar...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://verdant_cargo_321713_bigquery_to_pubsub_temp/staging/beamapp-root-0803114157-486015.1627990917.486127/dataflow_python_sdk.tar in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://verdant_cargo_321713_bigquery_to_pubsub_temp/staging/beamapp-root-0803114157-486015.1627990917.486127/apache_beam-2.25.0-cp37-cp37m-manylinux1_x86_64.whl...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://verdant_cargo_321713_bigquery_to_pubsub_temp/staging/beamapp-root-0803114157-486015.1627990917.486127/apache_beam-2.25.0-cp37-cp37m-manylinux1_x86_64.whl in 1 seconds.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-f68f5338-cb52-4a70-afa2-84c4ef63982a.json']\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-f68f5338-cb52-4a70-afa2-84c4ef63982a.json']\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://verdant_cargo_321713_bigquery_to_pubsub_temp/templates/CUSTOM_TEMPLATE_DATAFLOW_STREAMING...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://verdant_cargo_321713_bigquery_to_pubsub_temp/templates/CUSTOM_TEMPLATE_DATAFLOW_STREAMING in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:A template was just created at location gs://verdant_cargo_321713_bigquery_to_pubsub_temp/templates/CUSTOM_TEMPLATE_DATAFLOW_STREAMING\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<DataflowPipelineResult None at 0x7fc464665c50>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class KeepDoFn(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        import pandas as pd\n",
    "\n",
    "        return [{\n",
    "            'timestamp': element['block_timestamp_truncated'],\n",
    "            'request': str(element['oracle_request_id']),\n",
    "            'symbol' : element['symbol'],\n",
    "            'rate' : element['rate'],  \n",
    "            'date_str' : pd.to_datetime(element['block_timestamp'], format='%Y-%m-%dT%H:%M:%S.%fZ').strftime(\"%Y%m%d%H%M%S\")\n",
    "    }]\n",
    "\n",
    "project = google.auth.default()[1]\n",
    "table = \"{}:temp_crypto_batch.streaming_timestamp\".format(project)\n",
    "schema = \"timestamp:TIMESTAMP, request:STRING, symbol:STRING, rate:FLOAT, date_str:STRING\"\n",
    "topic = \"projects/{}/topics/bigquery-to-pubsub-test0\".format(project)\n",
    "bucket = \"gs://{project}_bigquery_to_pubsub_temp\".format(project=project.replace(\"-\", \"_\"))\n",
    "streaming = True\n",
    "\n",
    "options = PipelineOptions(\n",
    "        streaming=True,\n",
    "        project=project,\n",
    "        region=\"us-central1\",\n",
    "        staging_location=\"%s/staging\" % bucket,\n",
    "        temp_location=\"%s/temp\" % bucket, \n",
    "        setup_file=\"./setup.py\", \n",
    "        template_location= \"gs://{project}_bigquery_to_pubsub_temp/templates/CUSTOM_TEMPLATE_DATAFLOW_STREAMING\".format(project=project.replace(\"-\", \"_\"))\n",
    ")\n",
    "\n",
    "#p = beam.Pipeline(\"DataFlowRunner\", argv=pipeline_parameters)\n",
    "p = beam.Pipeline(DataflowRunner(), options=options)\n",
    "\n",
    "pubsub = (p | \"Read Topic\" >> ReadFromPubSub(topic=topic)\n",
    "                | \"To Dict\" >> beam.Map(json.loads)\n",
    "                | \"Format\" >> beam.ParDo(KeepDoFn())\n",
    "                | \"Write To BigQuery\" >> WriteToBigQuery(table=table, schema=schema,\n",
    "                                  create_disposition=BigQueryDisposition.CREATE_NEVER,\n",
    "                                  write_disposition=BigQueryDisposition.WRITE_APPEND))\n",
    "    \n",
    "p.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "7. Apache Beam 2.25.0 for Python 3",
   "language": "python",
   "name": "7-apache-beam-2.25.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
